# –î–æ—Ä–æ–∂–Ω–∞—è –∫–∞—Ä—Ç–∞ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ AI-–ø–ª–∞—Ç—Ñ–æ—Ä–º—ã –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏

## –û–±–∑–æ—Ä

–î–∞–Ω–Ω—ã–π –¥–æ–∫—É–º–µ–Ω—Ç –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π –ø–ª–∞–Ω —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ AI-–ø–ª–∞—Ç—Ñ–æ—Ä–º—ã –∞–≤—Ç–æ–º–∞—Ç–∏–∑–∞—Ü–∏–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Ç–µ–∫—É—â–µ–≥–æ —Å—Ç–µ–∫–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º MCP-–∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤. –î–æ—Ä–æ–∂–Ω–∞—è –∫–∞—Ä—Ç–∞ –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä—É–µ—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ –∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–µ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.

**–¢–µ–∫—É—â–µ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ —Å—Ç–µ–∫–∞**: 70% –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã –≥–æ—Ç–æ–≤–æ, –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏  
**–¶–µ–ª–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏**: <50–º—Å –∑–∞–¥–µ—Ä–∂–∫–∞, ‚â•85% —Ç–æ—á–Ω–æ—Å—Ç—å AI, ‚â•80% –ø–æ–ø–∞–¥–∞–Ω–∏–π –≤ –∫—ç—à, ‚â•99.99% –≤—Ä–µ–º—è —Ä–∞–±–æ—Ç—ã

## –ê–Ω–∞–ª–∏–∑ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

### –¢–µ–∫—É—â–∏–π —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—á–µ—Å–∫–∏–π —Å—Ç–µ–∫

```mermaid
graph TB
    A[–ö–ª–∏–µ–Ω—Ç—Å–∫–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è] --> B[Nginx Gateway 1.29.1]
    B --> C[n8n 1.108.1 Workflow Engine]
    B --> D[Supabase Edge Runtime v1.75.0]
    C --> E[Redis 8.2.1 Vector Search]
    C --> F[PostgreSQL 15.14 + pgvector]
    D --> F
    E --> G[RediSearch + RedisJSON]
    F --> H[AI Functions & Storage]
    
    subgraph "–°–ª–æ–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"
        I[Prometheus 3.5.0]
        J[Grafana 12.1.1]
        K[Redis Insight]
    end
    
    subgraph "AI –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã"
        L[OpenAI GPT-5]
        M[Cohere Rerank v3]
        N[Ollama Local Models]
        O[OpenAI Embeddings]
    end
```

### –ú–∞—Ç—Ä–∏—Ü–∞ —Å—Ç–∞—Ç—É—Å–∞ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤

| –ö–æ–º–ø–æ–Ω–µ–Ω—Ç | –°—Ç–∞—Ç—É—Å | –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å | –î–æ—Å—Ç—É–ø–Ω—ã–µ MCP –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã | –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –ø—Ä–æ–±–ª–µ–º—ã |
|-----------|--------|------------|---------------------------|---------------------|
| **Redis 8.2.1** | üü° –ß–∞—Å—Ç–∏—á–Ω–æ | 60% | ‚úÖ 44 –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ | –ù–µ—Ç –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ |
| **Supabase Cloud** | üü¢ –•–æ—Ä–æ—à–æ | 85% | ‚úÖ 19 –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ | –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç —Ç–∞–±–ª–∏—Ü—ã –¥–æ—Ä–æ–∂–Ω–æ–π –∫–∞—Ä—Ç—ã |
| **n8n Workflows** | üî¥ –ö—Ä–∏—Ç–∏—á–Ω–æ | 0% | ‚úÖ 39 –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ | –ù–µ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã—Ö workflow |
| **Cohere Rerank** | ‚ùå –û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç | 0% | ‚ùå –†—É—á–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ | –ù–µ—Ç –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏–∏ API |
| **Security** | üü° –ë–∞–∑–æ–≤–∞—è | 40% | ‚úÖ –í—Å—Ç—Ä–æ–µ–Ω–Ω–∞—è | –ù—É–∂–Ω–∞ –ø—Ä–æ–¥–∞–∫—à–Ω-–∑–∞—â–∏—Ç–∞ |

## –§–∞–∑–∞ 1: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫—Ä–∏—Ç–∏—á–µ—Å–∫–æ–π –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã (–ù–µ–¥–µ–ª—è 1)

### –ó–∞–¥–∞—á–∞ 1.1: –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ Redis

**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç**: üî• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π  
**MCP –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã**: `redis` (44 –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –¥–æ—Å—Ç—É–ø–Ω–æ)  
**–¶–µ–ª—å**: HNSW –∏–Ω–¥–µ–∫—Å—ã —Å M=64, EF_CONSTRUCTION=300

#### –®–∞–≥–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

1. **–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞**
```bash
# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ MCP Redis –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞: create_vector_index_hash
–ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
- index_name: "vector_index"
- prefix: "doc:"
- vector_field: "embedding"
- dim: 1536
- distance_metric: "COSINE"
```

2. **–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ HNSW**
```redis
FT.CREATE vector_index ON HASH PREFIX 1 doc: 
SCHEMA embedding VECTOR HNSW 6 
TYPE FLOAT32 DIM 1536 DISTANCE_METRIC COSINE 
M 64 EF_CONSTRUCTION 300 EF_RUNTIME 75
```

3. **–ö–æ–º–∞–Ω–¥—ã –≤–∞–ª–∏–¥–∞—Ü–∏–∏**
```bash
# MCP Redis –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:
- get_index_info("vector_index")
- get_indexed_keys_number("vector_index")
- vector_search_hash(test_vector, "vector_index")
```

#### –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
- –ó–∞–¥–µ—Ä–∂–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞ –º–µ–Ω–µ–µ 50–º—Å
- HNSW –∏–Ω–¥–µ–∫—Å –≤ —Ä–∞–±–æ—Ç–µ
- –û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏ Redis

### –ó–∞–¥–∞—á–∞ 1.2: –£–ª—É—á—à–µ–Ω–∏–µ —Å—Ö–µ–º—ã Supabase

**–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç**: üî• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π  
**MCP –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã**: `supabase` (19 –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–æ—Å—Ç—É–ø–Ω–æ)  
**–¶–µ–ª—å**: –ó–∞–≤–µ—Ä—à–∏—Ç—å —Å—Ö–µ–º—É –¥–æ—Ä–æ–∂–Ω–æ–π –∫–∞—Ä—Ç—ã v3.2

#### –®–∞–≥–∏ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏

1. **–°–æ–∑–¥–∞–Ω–∏–µ –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏—Ö —Ç–∞–±–ª–∏—Ü**
```sql
-- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ MCP Supabase –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞: execute_sql
CREATE TABLE ai_documents (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  content TEXT NOT NULL,
  metadata JSONB DEFAULT '{}',
  embedding vector(1536),
  contextual_fts TSVECTOR GENERATED ALWAYS AS 
    (to_tsvector('english', content)) STORED,
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE TABLE ai_conversations (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  session_id UUID NOT NULL,
  user_message TEXT,
  ai_response TEXT,
  metadata JSONB DEFAULT '{}',
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE TABLE ai_metrics (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  metric_type TEXT NOT NULL,
  metric_value FLOAT NOT NULL,
  metadata JSONB DEFAULT '{}',
  recorded_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);
```

2. **Create Optimized Indices**
```sql
-- HNSW vector index with optimized parameters
CREATE INDEX ai_documents_embedding_hnsw_idx 
ON ai_documents USING hnsw (embedding vector_cosine_ops)
WITH (m = 64, ef_construction = 300);

-- GIN index for metadata searches
CREATE INDEX ai_documents_metadata_gin 
ON ai_documents USING gin (metadata);

-- Full-text search index
CREATE INDEX ai_documents_fts_idx 
ON ai_documents USING gin (contextual_fts);
```

3. **RLS Policies**
```sql
-- Using MCP Supabase tool: execute_sql
ALTER TABLE ai_documents ENABLE ROW LEVEL SECURITY;

CREATE POLICY "user_documents_policy" ON ai_documents
  FOR ALL USING (
    (metadata->>'user_id')::uuid = auth.uid()
  );
```

#### Validation
```bash
# MCP Supabase tools to use:
- list_tables() # Verify table creation
- execute_sql("SELECT * FROM ai_documents LIMIT 1") # Test access
```

### Task 1.3: n8n Workflow Database Connection

**Priority**: üî• Critical  
**MCP Tools**: `n8n-mcp` (39 tools available)  
**Target**: Restore workflow functionality

#### Implementation Steps

1. **Database Health Check**
```bash
# Using MCP n8n tools:
- n8n_health_check()
- get_database_statistics()
- n8n_diagnostic()
```

2. **Workflow Import/Creation**
```bash
# MCP n8n tools workflow:
- list_templates() # Find AI workflow templates
- get_templates_for_task("vector_search") # Get vector search workflows
- n8n_create_workflow(workflow_config) # Create basic workflows
```

3. **Node Configuration**
```bash
# Configure AI-specific nodes:
- search_nodes("redis") # Find Redis nodes
- search_nodes("supabase") # Find Supabase nodes
- get_node_for_task("vector_similarity_search") # Get pre-configured nodes
```

#### Expected Outcomes
- n8n database connectivity restored
- Basic AI workflows operational
- MCP tools returning valid statistics

## Phase 2: AI Integration & Optimization (Week 2)

### Task 2.1: Cohere Rerank API Integration

**Priority**: üìà High  
**MCP Tools**: Manual configuration required  
**Target**: ‚â•85% search accuracy

#### Implementation Steps

1. **API Configuration**
```javascript
// Environment setup
const COHERE_API_KEY = process.env.COHERE_API_KEY;
const COHERE_BASE_URL = 'https://api.cohere.ai/v2';

// n8n HTTP Request Node configuration
{
  "url": "https://api.cohere.ai/v2/rerank",
  "method": "POST",
  "headers": {
    "Authorization": "Bearer {{ $vars.COHERE_API_KEY }}",
    "Content-Type": "application/json"
  },
  "body": {
    "model": "rerank-english-v3.0",
    "query": "{{ $json.query }}",
    "documents": "{{ $json.documents }}",
    "top_k": 10
  }
}
```

2. **Workflow Integration**
```bash
# Using MCP n8n tools:
- search_nodes("http") # Find HTTP request nodes
- get_node_for_task("api_request") # Get API request template
- validate_node_operation(cohere_config) # Validate configuration
```

3. **Caching Implementation**
```bash
# Redis caching for Cohere results
# Using MCP Redis tools:
- set("rerank:cache:{hash}", results, "EX", 86400)
- get("rerank:cache:{hash}") # Check cache before API call
```

#### Expected Outcomes
- Cohere reranking operational
- 35% latency reduction through caching
- Cost monitoring implemented

### Task 2.2: Contextual Retrieval Enhancement

**Priority**: üìà High  
**MCP Tools**: `supabase` + `n8n-mcp`  
**Target**: Improved search relevance

#### Implementation Steps

1. **Database Schema Update**
```sql
-- Using MCP Supabase tool: execute_sql
ALTER TABLE documents_v2 
ADD COLUMN contextual_content TEXT,
ADD COLUMN contextual_fts TSVECTOR 
GENERATED ALWAYS AS (to_tsvector('english', contextual_content)) STORED;
```

2. **Hybrid Search Function**
```sql
-- Using MCP Supabase tool: execute_sql
CREATE OR REPLACE FUNCTION hybrid_search_v4(
  query_embedding vector(1536),
  search_text TEXT DEFAULT '',
  contextual_weight FLOAT DEFAULT 0.8,
  max_results INT DEFAULT 10
) RETURNS TABLE (
  id UUID,
  content TEXT,
  metadata JSONB,
  similarity FLOAT,
  text_rank FLOAT,
  final_score FLOAT
);
```

3. **n8n Workflow Update**
```bash
# Using MCP n8n tools:
- get_templates_for_task("contextual_search") # Get contextual search template
- n8n_update_partial_workflow(workflow_id, contextual_nodes) # Add contextual nodes
```

### Task 2.3: LightRAG Integration

**Priority**: üìà High  
**MCP Tools**: `supabase` (Edge Functions)  
**Target**: Graph-based knowledge retrieval

#### Implementation Steps

1. **Edge Function Creation**
```typescript
// Using MCP Supabase tool: deploy_edge_function
// File: supabase/functions/light-rag/index.ts
import { LightRAG } from 'lightrag';

export default async function handler(req: Request) {
  const { query, documents, mode = 'hybrid' } = await req.json();
  
  const lightrag = new LightRAG({
    supabaseConfig: {
      url: Deno.env.get('SUPABASE_URL'),
      key: Deno.env.get('SUPABASE_SERVICE_KEY')
    }
  });
  
  const results = await lightrag.process({
    query,
    documents,
    mode
  });
  
  return new Response(JSON.stringify(results), {
    headers: { 'Content-Type': 'application/json' }
  });
}
```

2. **n8n Integration**
```bash
# Using MCP n8n tools:
- search_nodes("supabase") # Find Supabase nodes
- get_node_for_task("edge_function_call") # Get edge function template
```

## Phase 3: Performance Optimization (Week 3)

### Task 3.1: Redis Memory Optimization

**Priority**: üîß Medium  
**MCP Tools**: `redis` (44 tools available)  
**Target**: Memory fragmentation <2.0

#### Implementation Steps

1. **Memory Analysis**
```bash
# Using MCP Redis tools:
- info("memory") # Current memory stats
- dbsize() # Database size
- scan_all_keys() # Key analysis
```

2. **Configuration Optimization**
```redis
# Redis configuration updates
CONFIG SET io-threads 8
CONFIG SET save "900 1 300 10 60 10000"
CONFIG SET maxmemory-policy allkeys-lru
```

3. **Monitoring Setup**
```bash
# Using MCP Redis tools:
- client_list() # Monitor connections
- info("stats") # Performance statistics
```

### Task 3.2: Supabase Query Optimization

**Priority**: üîß Medium  
**MCP Tools**: `supabase` (19 tools available)  
**Target**: <100ms query response time

#### Implementation Steps

1. **Index Analysis**
```sql
-- Using MCP Supabase tool: execute_sql
SELECT 
  indexname,
  tablename,
  pg_size_pretty(pg_relation_size(indexname::regclass)) as size
FROM pg_indexes 
WHERE tablename LIKE 'ai_%';
```

2. **Query Performance Testing**
```sql
-- Using MCP Supabase tool: execute_sql
EXPLAIN (ANALYZE, BUFFERS) 
SELECT * FROM ai_documents 
WHERE embedding <=> '[0.1,0.2,...]'::vector 
ORDER BY embedding <=> '[0.1,0.2,...]'::vector 
LIMIT 10;
```

## Phase 4: Security & Production Readiness (Week 4)

### Task 4.1: Security Hardening

**Priority**: üîß Medium  
**MCP Tools**: `redis`, `supabase`  
**Target**: Production-grade security

#### Implementation Steps

1. **Redis ACL Configuration**
```bash
# Using MCP Redis tools:
ACL SETUSER n8n on >password ~cached:* ~ai:* +@read +@write +eval +ft.*
ACL SETUSER readonly on >password ~* +@read -@dangerous
```

2. **Supabase RLS Enhancement**
```sql
-- Using MCP Supabase tool: execute_sql
CREATE POLICY "enhanced_user_isolation" ON ai_documents
  FOR ALL USING (
    auth.uid() IS NOT NULL AND
    (metadata->>'user_id')::uuid = auth.uid()
  );
```

### Task 4.2: Monitoring & Alerting

**Priority**: üîß Medium  
**MCP Tools**: `redis`, `supabase`  
**Target**: 99.99% uptime monitoring

#### Implementation Steps

1. **Health Check Endpoints**
```bash
# Using MCP tools for monitoring:
- redis: info("server") # Redis health
- supabase: execute_sql("SELECT 1") # Database health
- n8n: n8n_health_check() # Workflow health
```

2. **Metrics Collection**
```bash
# Key metrics to track using MCP tools:
- Vector search latency (redis: vector_search_hash timing)
- Database query performance (supabase: query execution time)
- Workflow success rate (n8n: execution statistics)
```

## Implementation Timeline

### Week 1: Critical Infrastructure
- **Days 1-2**: Redis vector indices creation
- **Days 3-4**: Supabase schema enhancement  
- **Days 5-7**: n8n workflow restoration

### Week 2: AI Integration
- **Days 8-10**: Cohere API integration
- **Days 11-12**: Contextual retrieval
- **Days 13-14**: LightRAG implementation

### Week 3: Performance Optimization
- **Days 15-17**: Redis optimization
- **Days 18-19**: Database tuning
- **Days 20-21**: Load testing

### Week 4: Production Readiness
- **Days 22-24**: Security hardening
- **Days 25-26**: Monitoring setup
- **Days 27-28**: Final testing & deployment

## MCP Tools Usage Strategy

### Daily Operations
```bash
# Redis Management
redis: get_index_info, vector_search_hash, info

# Supabase Operations  
supabase: execute_sql, list_tables, get_logs

# n8n Workflow Management
n8n-mcp: n8n_health_check, list_workflows, validate_workflow

# Advanced Analysis
sequential-thinking: For complex problem-solving
context7: For documentation lookup
```

### Validation & Testing
```bash
# Performance Testing
redis: vector_search_hash(test_vectors) # Latency testing
supabase: execute_sql(performance_queries) # Query optimization
n8n-mcp: n8n_list_executions() # Workflow performance

# Health Monitoring
redis: info("stats") # Redis statistics
supabase: get_logs() # Error monitoring  
n8n-mcp: get_database_statistics() # Workflow statistics
```

## Expected Outcomes

### Performance Targets
| Metric | Current | Target | Week 4 Expected |
|--------|---------|--------|----|
| **Search Latency** | N/A | <50ms | 42-48ms |
| **AI Accuracy** | N/A | ‚â•85% | 88-90% |
| **Cache Hit Rate** | 75% | ‚â•80% | 82-85% |
| **System Uptime** | 99.9% | ‚â•99.99% | 99.99% |

### Component Readiness
- **Redis 8.2.1**: 60% ‚Üí 95% (Vector indices + optimization)
- **Supabase Cloud**: 85% ‚Üí 98% (Complete schema + RLS)
- **n8n Workflows**: 0% ‚Üí 90% (AI automation pipelines)
- **Cohere Rerank**: 0% ‚Üí 85% (API integration + caching)
- **Security**: 40% ‚Üí 85% (Production hardening)

## Risk Mitigation

### Technical Risks
- **Redis Memory Issues**: Continuous monitoring with MCP tools
- **Supabase Query Performance**: Regular optimization with execute_sql
- **n8n Connectivity**: Health checks using n8n_health_check

### Operational Risks
- **API Rate Limits**: Caching strategies for Cohere
- **Data Loss**: Backup validation using MCP tools
- **Performance Degradation**: Real-time monitoring setup

This roadmap leverages MCP tools for hands-on implementation, validation, and monitoring throughout the development process, ensuring each phase builds upon verified foundations.

## üìÅ –û–ë–ù–ê–†–£–ñ–ï–ù–ù–´–ï –ü–û–õ–ï–ó–ù–´–ï –§–ê–ô–õ–´ –ü–†–û–ï–ö–¢–ê

### üîß –ì–æ—Ç–æ–≤–∞—è –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä–∞ –≤ –ø—Ä–æ–µ–∫—Ç–µ:

**üî¥ MCP Redis Server (–ø–æ–ª–Ω–æ—Å—Ç—å—é –≥–æ—Ç–æ–≤)**
- **–ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ**: `mcp-redis/` –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è
- **–°—Ç–∞—Ç—É—Å**: ‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç (–≤–µ—Ä—Å–∏—è 0.3.0)
- **–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è**: –£–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –≤ `.mcp.json` —Å 44 –¥–æ—Å—Ç—É–ø–Ω—ã–º–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
- **–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ**: `redis://default:redis_secure_password_2024_domain@95.111.252.29:6379/0`
- **–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏**: –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫, HNSW –∏–Ω–¥–µ–∫—Å—ã, JSON –æ–ø–µ—Ä–∞—Ü–∏–∏, Pub/Sub

**üîµ Supabase Edge Functions (–≥–æ—Ç–æ–≤—ã–µ —à–∞–±–ª–æ–Ω—ã)**
- **–ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ**: `supabase/functions/`
- **–°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏**:
  - `generate-product-embedding/index.ts` - –≥–µ–Ω–µ—Ä–∞—Ü–∏—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø—Ä–æ–¥—É–∫—Ç–æ–≤
  - `generate-product-tags/` - –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–≥–∏
- **–í–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏**: OpenAI API –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è, –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏

**üü¢ n8n Workflow –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è (–¥–µ—Ç–∞–ª—å–Ω—ã–µ –ø–ª–∞–Ω—ã)**
- **–ú–µ—Å—Ç–æ–ø–æ–ª–æ–∂–µ–Ω–∏–µ**: `n8n/` –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è  
- **–ö–ª—é—á–µ–≤—ã–µ —Ñ–∞–π–ª—ã**:
  - `workflow-task.md` - –¥–µ—Ç–∞–ª—å–Ω—ã–π —Å–ø–∏—Å–æ–∫ –∏–∑ 46 –∑–∞–¥–∞—á —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏
  - `current-stack-state.md` - –∞–Ω–∞–ª–∏–∑ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ—Å—Ç–æ—è–Ω–∏—è (70% –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏)
  - `combined-implementation-roadmap.md` - –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π –ø–ª–∞–Ω
- **–°–æ—Å—Ç–æ—è–Ω–∏–µ**: Redis –∏ Supabase –≥–æ—Ç–æ–≤—ã, n8n —Ç—Ä–µ–±—É–µ—Ç –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è

**üìã –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã**
- **`.mcp.json`**: –ü–æ–ª–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ MCP —Å–µ—Ä–≤–µ—Ä–æ–≤ —Å —É—á–µ—Ç–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
- **`mcp-redis/pyproject.toml`**: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è Redis MCP —Å–µ—Ä–≤–µ—Ä–∞
- **`SUPABASE_COMPREHENSIVE_GUIDE.md`**: 688 —Å—Ç—Ä–æ–∫ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –ë–î

### üéØ –ö–ª—é—á–µ–≤—ã–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è –¥–ª—è –ø–ª–∞–Ω–∞:

**‚úÖ –ß—Ç–æ —É–∂–µ —Ä–∞–±–æ—Ç–∞–µ—Ç:**
1. **Redis MCP Server**: –ü–æ–ª–Ω–æ—Å—Ç—å—é –Ω–∞—Å—Ç—Ä–æ–µ–Ω –∏ –≥–æ—Ç–æ–≤ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é
2. **Supabase Schema**: –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö —Å —Ç–∞–±–ª–∏—Ü–∞–º–∏ `documents_v2`, `opencart_products`, –≤–µ–∫—Ç–æ—Ä–Ω—ã–º–∏ –∏–Ω–¥–µ–∫—Å–∞–º–∏
3. **Edge Functions**: –ì–æ—Ç–æ–≤—ã–µ —à–∞–±–ª–æ–Ω—ã –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤

**üî• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–µ –±–ª–æ–∫–µ—Ä—ã –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω—ã:**
1. **n8n connectivity**: –ù–µ—Ç —Ä–∞–±–æ—á–∏—Ö workflow (0 —É–∑–ª–æ–≤)
2. **–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤**: Redis –ø—É—Å—Ç (0 –∫–ª—é—á–µ–π)
3. **Cohere –Ω–µ –∏–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞–Ω**: –ù–µ—Ç API –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è

**üìà –ì–æ—Ç–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:**
- –ì–æ—Ç–æ–≤—ã–π MCP Redis —Å–µ—Ä–≤–µ—Ä —Å 44 –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
- –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ Supabase Edge Functions –∫–∞–∫ —à–∞–±–ª–æ–Ω—ã
- –î–µ—Ç–∞–ª—å–Ω—ã–µ –∑–∞–¥–∞—á–∏ –≤ workflow-task.md (46 –∑–∞–¥–∞—á —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏)

## ü§ñ –ü–†–ê–ö–¢–ò–ß–ï–°–ö–ò–ô –ü–õ–ê–ù –í–´–ü–û–õ–ù–ï–ù–ò–Ø –î–õ–Ø AI –ê–°–°–ò–°–¢–ï–ù–¢–ê

### –î–æ—Å—Ç—É–ø–Ω—ã–µ MCP –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –¥–ª—è —Ä–∞–±–æ—Ç—ã:

**üî¥ Redis MCP (44 –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞)**
```bash
# –û—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏:
- info() # –û–±—â–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ Redis
- get_indexes() # –°–ø–∏—Å–æ–∫ –≤—Å–µ—Ö –∏–Ω–¥–µ–∫—Å–æ–≤
- create_vector_index_hash() # –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –∏–Ω–¥–µ–∫—Å–∞
- vector_search_hash() # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
- get_index_info() # –î–µ—Ç–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ–± –∏–Ω–¥–µ–∫—Å–µ
- scan_all_keys() # –ê–Ω–∞–ª–∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∫–ª—é—á–µ–π
```

**üîµ Supabase MCP (19 –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤) - –ù–ê–°–¢–†–û–ï–ù**
```bash
# –ü—Ä–æ–≤–µ—Ä–µ–Ω–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã –∏–∑ .mcp.json:
# Command: npx -y @supabase/mcp-server-supabase@latest --project-ref=bvcgsavjmrvkxcetyeyz
# Project: bvcgsavjmrvkxcetyeyz.supabase.co

# === DATABASE MANAGEMENT (8 tools) ===
- list_tables            # List all tables in database
- list_extensions        # List installed PostgreSQL extensions
- execute_sql            # Execute SQL queries directly
- get_logs               # Get database logs for debugging
- get_advisors           # Get optimization recommendations
- get_project_url        # Get Supabase project URL
- get_anon_key           # Get anonymous access key
- generate_typescript_types # Generate TypeScript types from schema

# === BRANCH MANAGEMENT (6 tools) ===
- create_branch          # Create database branch
- list_branches          # List all database branches
- delete_branch          # Delete database branch
- merge_branch           # Merge database branch
- reset_branch           # Reset database branch
- rebase_branch          # Rebase database branch

# === MIGRATION MANAGEMENT (2 tools) ===
- list_migrations        # List database migrations
- apply_migration        # Apply pending migrations

# === EDGE FUNCTIONS (2 tools) ===
- list_edge_functions    # List deployed Edge Functions
- deploy_edge_function   # Deploy new Edge Function

# === DOCUMENTATION (1 tool) ===
- search_docs            # Search Supabase documentation

# –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ Edge Functions –≤ –ø—Ä–æ–µ–∫—Ç–µ:
# - generate-product-embedding (OpenAI text-embedding-3-small)
# - generate-product-tags (–∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ–≥–∏)
```

**üü¢ n8n MCP (39 –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤) - –¢–†–ï–ë–£–ï–¢ –ü–û–î–ö–õ–Æ–ß–ï–ù–ò–Ø**
```bash
# Command: npx -y n8n-mcp
# Status: 0 nodes available (–∫—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –ø—Ä–æ–±–ª–µ–º–∞)

# === CORE TOOLS (1 tool) ===
- tools_documentation    # Get MCP tool documentation (START HERE!)

# === NODE DISCOVERY (4 tools) ===
- list_nodes             # List all n8n nodes with filtering options
- search_nodes           # Search nodes by keyword/functionality
- list_ai_tools          # List AI-capable nodes (268 detected)
- get_database_statistics # Get workflow database statistics

# === NODE CONFIGURATION (8 tools) ===
- get_node_info          # Get complete node schema (100KB+)
- get_node_essentials    # Get compact node overview (5KB)
- search_node_properties # Search for specific node properties
- get_node_for_task      # Get pre-configured node templates
- get_property_dependencies # Get node property dependencies
- get_node_as_tool_info  # Get node information as AI tool
- list_node_templates    # List available node templates
- get_template           # Get specific template

# === TASK MANAGEMENT (3 tools) ===
- list_tasks             # List available workflow tasks
- search_templates       # Search workflow templates
- get_templates_for_task # Get templates for specific tasks

# === VALIDATION TOOLS (4 tools) ===
- validate_node_operation # Full node validation with fixes
- validate_node_minimal  # Required fields validation only
- validate_workflow      # Complete workflow validation
- validate_workflow_connections # Validate workflow connections
- validate_workflow_expressions # Validate workflow expressions

# –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –ü–†–û–ë–õ–ï–ú–ê: get_database_statistics() –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 0 nodes
# –ù–ï–û–ë–•–û–î–ò–ú–û: –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ n8n database
# –î–û–°–¢–£–ü–ù–´–ï –†–ï–®–ï–ù–ò–Ø: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–æ—Ç–æ–≤—ã–µ –ø–ª–∞–Ω—ã –∏–∑ workflow-task.md (46 –∑–∞–¥–∞—á)
```

### –ü–û–®–ê–ì–û–í–´–ô –ß–ï–ö-–õ–ò–°–¢ –í–´–ü–û–õ–ù–ï–ù–ò–Ø

#### üî• –î–ï–ù–¨ 1: –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∏ –≤–∞–ª–∏–¥–∞—Ü–∏—è –≥–æ—Ç–æ–≤–æ–π –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã
**–ú–æ–∏ –¥–µ–π—Å—Ç–≤–∏—è:**
- [ ] `redis: info()` - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å Redis 8.2.1 (–æ–∂–∏–¥–∞–µ—Ç—Å—è: —Ä–∞–±–æ—Ç–∞–µ—Ç, 0 –∫–ª—é—á–µ–π)
- [ ] `redis: get_indexes()` - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –∏–Ω–¥–µ–∫—Å—ã (–æ–∂–∏–¥–∞–µ—Ç—Å—è: –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫)
- [ ] `supabase: list_tables()` - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –≥–æ—Ç–æ–≤—ã–µ —Ç–∞–±–ª–∏—Ü—ã (documents_v2, opencart_products)
- [ ] `supabase: execute_sql("SELECT COUNT(*) FROM documents_v2")` - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–∞–Ω–Ω—ã–µ
- [ ] `n8n: n8n_health_check()` - –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ n8n connectivity
- [ ] `n8n: get_database_statistics()` - –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É (–æ–∂–∏–¥–∞–µ—Ç—Å—è: 0 nodes)
- [ ] –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –≥–æ—Ç–æ–≤—ã–µ Edge Functions –≤ Supabase
- [ ] –°–æ–∑–¥–∞—Ç—å –æ—Ç—á–µ—Ç –æ —Ä–µ–∞–ª—å–Ω–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏ vs –æ–∂–∏–¥–∞–µ–º–æ–º

#### üî• –î–ï–ù–¨ 2: –°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ Redis —Å –≥–æ—Ç–æ–≤–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π
**–ú–æ–∏ –¥–µ–π—Å—Ç–≤–∏—è:**
- [ ] `redis: create_vector_index_hash(index_name="idx_documents", prefix="doc:", vector_field="embedding", dim=1536, distance_metric="COSINE")`
- [ ] `redis: create_vector_index_hash(index_name="idx_products", prefix="product:", vector_field="embedding", dim=1536, distance_metric="COSINE")`
- [ ] `redis: get_index_info("idx_documents")` - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–æ–∑–¥–∞–Ω–∏–µ
- [ ] `redis: get_index_info("idx_products")` - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å–æ–∑–¥–∞–Ω–∏–µ
- [ ] –°–æ–∑–¥–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—ã–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ Supabase `documents_v2`
- [ ] –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å —Å `vector_search_hash()`
- [ ] –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞—Ç—å —Å–æ–∑–¥–∞–Ω–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã

#### üî• –î–ï–ù–¨ 3-4: –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≥–æ—Ç–æ–≤–æ–π —Å—Ö–µ–º—ã Supabase
**–ú–æ–∏ –¥–µ–π—Å—Ç–≤–∏—è:**
- [ ] `supabase: list_tables()` - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –≥–æ—Ç–æ–≤—ã–µ —Ç–∞–±–ª–∏—Ü—ã (documents_v2, opencart_products –∏ –¥—Ä.)
- [ ] `supabase: execute_sql()` –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–µ–¥–æ—Å—Ç–∞—é—â–µ–π —Ç–∞–±–ª–∏—Ü—ã `ai_conversations`
- [ ] `supabase: execute_sql()` –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –Ω–µ–¥–æ—Å—Ç–∞—é—â–µ–π —Ç–∞–±–ª–∏—Ü—ã `ai_metrics`
- [ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –∏ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ HNSW –∏–Ω–¥–µ–∫—Å—ã
- [ ] `supabase: execute_sql("ALTER TABLE documents_v2 ADD COLUMN contextual_fts TSVECTOR")` –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø–æ–∏—Å–∫–∞
- [ ] –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≥–æ—Ç–æ–≤—ã–µ RLS –ø–æ–ª–∏—Ç–∏–∫–∏
- [ ] –í–∞–ª–∏–¥–∏—Ä–æ–≤–∞—Ç—å –≥–æ—Ç–æ–≤—ã–µ Edge Functions –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
- [ ] `supabase: list_edge_functions()` - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏

#### üî• –î–ï–ù–¨ 5-7: –í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ n8n –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–æ—Ç–æ–≤—ã—Ö –ø–ª–∞–Ω–æ–≤
**–ú–æ–∏ –¥–µ–π—Å—Ç–≤–∏—è:**
- [ ] `n8n: list_workflows()` - –ø—Ä–æ–≤–µ—Ä–∏—Ç—å —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ workflow (–æ–∂–∏–¥–∞–µ—Ç—Å—è: –ø—É—Å—Ç–æ–π)
- [ ] `n8n: search_nodes("redis")` - –Ω–∞–π—Ç–∏ Redis —É–∑–ª—ã –¥–ª—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è
- [ ] `n8n: search_nodes("supabase")` - –Ω–∞–π—Ç–∏ Supabase —É–∑–ª—ã  
- [ ] –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–æ—Ç–æ–≤—ã–µ –ø–ª–∞–Ω—ã –∏–∑ `workflow-task.md` (46 –∑–∞–¥–∞—á)
- [ ] –°–æ–∑–¥–∞—Ç—å workflow "AI-Test Processing" –Ω–∞ –æ—Å–Ω–æ–≤–µ `combined-implementation-roadmap.md`
- [ ] –ò–Ω—Ç–µ–≥—Ä–∏—Ä–æ–≤–∞—Ç—å –≥–æ—Ç–æ–≤—ã–µ Edge Functions –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
- [ ] –ù–∞—Å—Ç—Ä–æ–∏—Ç—å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Redis MCP —Å–µ—Ä–≤–µ—Ä—É
- [ ] –í–∞–ª–∏–¥–∞—Ü–∏—è —á–µ—Ä–µ–∑ `validate_workflow()` –∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

### –®–ê–ë–õ–û–ù–´ –ö–û–ú–ê–ù–î –î–õ–Ø –ë–´–°–¢–†–û–ì–û –í–´–ü–û–õ–ù–ï–ù–ò–Ø (–ù–ê –û–°–ù–û–í–ï –ì–û–¢–û–í–û–ô –ò–ù–§–†–ê–°–¢–†–£–ö–¢–£–†–´)

#### –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–æ–≥–æ Redis MCP —Å–µ—Ä–≤–µ—Ä–∞:
```python
# –ö–æ–º–∞–Ω–¥—ã –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è —Å –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ–º –∏–∑ .mcp.json:
create_vector_index_hash(
    index_name="idx_documents",
    prefix="doc:",
    vector_field="embedding",
    dim=1536,
    distance_metric="COSINE"
)

# –°–æ–∑–¥–∞–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–∞ –¥–ª—è –ø—Ä–æ–¥—É–∫—Ç–æ–≤ (–∏—Å–ø–æ–ª—å–∑—É—è —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é —Ç–∞–±–ª–∏—Ü—É opencart_products)
create_vector_index_hash(
    index_name="idx_products", 
    prefix="product:",
    vector_field="embedding",
    dim=1536,
    distance_metric="COSINE"
)
```

#### –†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≥–æ—Ç–æ–≤–æ–π —Å—Ö–µ–º—ã Supabase:
```sql
-- –ö–æ–º–∞–Ω–¥–∞ execute_sql() –¥–ª—è –¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö —Ç–∞–±–ª–∏—Ü:
-- (documents_v2 –∏ opencart_products —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç —Å pgvector)

CREATE TABLE ai_conversations (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  session_id UUID NOT NULL,
  user_message TEXT,
  ai_response TEXT,
  metadata JSONB DEFAULT '{}',
  created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

CREATE TABLE ai_metrics (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  metric_type TEXT NOT NULL,
  metric_value FLOAT NOT NULL,
  metadata JSONB DEFAULT '{}',
  recorded_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
);

-- –£–ª—É—á—à–µ–Ω–∏–µ contextual retrieval –¥–ª—è —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π documents_v2
ALTER TABLE documents_v2 
ADD COLUMN contextual_content TEXT,
ADD COLUMN contextual_fts TSVECTOR 
GENERATED ALWAYS AS (to_tsvector('english', contextual_content)) STORED;
```

#### –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π Edge Function –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–æ—Ç–æ–≤–æ–≥–æ —à–∞–±–ª–æ–Ω–∞:
```typescript
// –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–æ–≥–æ —à–∞–±–ª–æ–Ω–∞ –∏–∑ generate-product-embedding/index.ts
// –ö–æ–º–∞–Ω–¥–∞ deploy_edge_function() –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è light-rag —Ñ—É–Ω–∫—Ü–∏–∏:

import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import OpenAI from 'https://deno.land/x/openai@v4.24.0/mod.ts';

serve(async (req) => {
  const { query, documents, mode = 'hybrid' } = await req.json();
  
  // –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–æ—Ç–æ–≤—É—é OpenAI –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –∏–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Ñ—É–Ω–∫—Ü–∏–∏
  const apiKey = Deno.env.get('OPENAI_API_KEY');
  const openai = new OpenAI({ apiKey });
  
  // LightRAG –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–æ—Ç–æ–≤–æ–π –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã
  const results = await processLightRAG(query, documents, mode);
  
  return new Response(JSON.stringify(results), {
    headers: { "Content-Type": "application/json" }
  });
});
```

### –ü–†–ò–û–†–ò–¢–ï–¢–´ –ò –ü–û–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–ù–û–°–¢–¨ (–û–ë–ù–û–í–õ–ï–ù–û –ù–ê –û–°–ù–û–í–ï –ê–ù–ê–õ–ò–ó–ê –ü–†–û–ï–ö–¢–ê)

**üö® –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏–π –ø—É—Ç—å (–±–ª–æ–∫–∏—Ä—É—é—â–∏–µ –∑–∞–¥–∞—á–∏) - –ü–û–î–¢–í–ï–†–ñ–î–ï–ù–û –ê–ù–ê–õ–ò–ó–û–ú:**
1. **–í–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ n8n –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è** - –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ: 0 nodes –≤ get_database_statistics()
2. **–°–æ–∑–¥–∞–Ω–∏–µ –≤–µ–∫—Ç–æ—Ä–Ω—ã—Ö –∏–Ω–¥–µ–∫—Å–æ–≤ Redis** - –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ: Redis —Ä–∞–±–æ—Ç–∞–µ—Ç, –Ω–æ –ø—É—Å—Ç–æ–π (0 –∫–ª—é—á–µ–π)
3. **–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å –≥–æ—Ç–æ–≤—ã–º–∏ Supabase —Ç–∞–±–ª–∏—Ü–∞–º–∏** - documents_v2 –∏ opencart_products –≥–æ—Ç–æ–≤—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é

**üìà –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç (—É–ª—É—á—à–µ–Ω–∏—è) - –ì–û–¢–û–í–´–ï –ö–û–ú–ü–û–ù–ï–ù–¢–´:**
4. **Cohere API –∏–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è** - –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–æ—Ç–æ–≤—ã–µ –ø–ª–∞–Ω—ã –∏–∑ workflow-task.md
5. **–†–∞—Å—à–∏—Ä–µ–Ω–∏–µ –≥–æ—Ç–æ–≤—ã—Ö Edge Functions** - —à–∞–±–ª–æ–Ω—ã —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É—é—Ç –≤ supabase/functions/
6. **–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö HNSW –∏–Ω–¥–µ–∫—Å–æ–≤** - –≤ documents_v2 —É–∂–µ –µ—Å—Ç—å –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –∏–Ω–¥–µ–∫—Å—ã

**üîß –ó–∞–¥–∞—á–∏ –¥–ª—è –≥–æ—Ç–æ–≤–æ–π –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã:**
7. **–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –≥–æ—Ç–æ–≤–æ–≥–æ MCP Redis —Å–µ—Ä–≤–µ—Ä–∞** - —É–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –≤ .mcp.json —Å 44 –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞–º–∏
8. **–ê–¥–∞–ø—Ç–∞—Ü–∏—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –ø–ª–∞–Ω–æ–≤** - 46 –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –∑–∞–¥–∞—á –≤ workflow-task.md
9. **–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏** - –≤ documents_v2 —É–∂–µ –µ—Å—Ç—å 4 –∂–∏–≤—ã–µ –∑–∞–ø–∏—Å–∏

### –¢–û–ß–ö–ò –ö–û–ù–¢–†–û–õ–Ø –ò –í–ê–õ–ò–î–ê–¶–ò–ò

**–ü–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —ç—Ç–∞–ø–∞ –ø—Ä–æ–≤–µ—Ä—è—é:**
- [ ] `redis: info()` - —Å–æ—Å—Ç–æ—è–Ω–∏–µ Redis
- [ ] `supabase: get_advisors()` - —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏
- [ ] `n8n: get_database_statistics()` - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ n8n
- [ ] –î–æ–∫—É–º–µ–Ω—Ç–∏—Ä—É—é —É—Å–ø–µ—à–Ω—ã–µ –∫–æ–º–∞–Ω–¥—ã –∏ –∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

### –ü–õ–ê–ù –ù–ê –°–õ–£–ß–ê–ô –ü–†–û–ë–õ–ï–ú (–û–ë–ù–û–í–õ–ï–ù–û –î–õ–Ø –ì–û–¢–û–í–û–ô –ò–ù–§–†–ê–°–¢–†–£–ö–¢–£–†–´)

**–ï—Å–ª–∏ Redis –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω:**
- –ü—Ä–æ–≤–µ—Ä–∏—Ç—å MCP —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∏–∑ .mcp.json: `redis://default:redis_secure_password_2024_domain@95.111.252.29:6379/0`
- Redis MCP Server –≤–µ—Ä—Å–∏–∏ 0.3.0 —É–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `sequential-thinking` –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–±–ª–µ–º—ã
- –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –ª–æ–≥–∏ MCP Redis —Å–µ—Ä–≤–µ—Ä–∞ –≤ `mcp-redis/` –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏

**–ï—Å–ª–∏ Supabase –Ω–µ –æ—Ç–≤–µ—á–∞–µ—Ç:**
- –ü—Ä–æ–≤–µ—Ä–∏—Ç—å `get_project_url()` - –¥–æ–ª–∂–µ–Ω –≤–µ—Ä–Ω—É—Ç—å `bvcgsavjmrvkxcetyeyz.supabase.co`  
- Supabase MCP —Å–µ—Ä–≤–µ—Ä —É–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω —Å project-ref –∏ access token
- –ü—Ä–æ–≤–µ—Ä–∏—Ç—å –≥–æ—Ç–æ–≤—ã–µ —Ç–∞–±–ª–∏—Ü—ã: documents_v2, opencart_products, user_query_history
- –û–±—Ä–∞—Ç–∏—Ç—å—Å—è –∫ SUPABASE_COMPREHENSIVE_GUIDE.md –∑–∞ –¥–µ—Ç–∞–ª—è–º–∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—ã

**–ï—Å–ª–∏ n8n –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç:**
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `n8n_diagnostic()` –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
- –ü—Ä–∏–º–µ–Ω–∏—Ç—å –≥–æ—Ç–æ–≤—ã–µ –ø–ª–∞–Ω—ã –∏–∑ n8n/workflow-task.md (46 –ø—Ä–∏–æ—Ä–∏—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –∑–∞–¥–∞—á)
- –í–æ—Å—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑—É—è n8n/current-stack-state.md –∞–Ω–∞–ª–∏–∑
- –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –≥–æ—Ç–æ–≤—ã–µ —à–∞–±–ª–æ–Ω—ã –∏–∑ n8n/combined-implementation-roadmap.md
- –ü—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ –ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å –±–∞–∑–æ–≤—ã–µ workflow –Ω–∞ –æ—Å–Ω–æ–≤–µ –≥–æ—Ç–æ–≤—ã—Ö –ø–ª–∞–Ω–æ–≤

**–ü—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞ –≥–æ—Ç–æ–≤–æ–π –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã:**
- MCP —Å–µ—Ä–≤–µ—Ä—ã —É–∂–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω—ã –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω—ã
- –î–µ—Ç–∞–ª—å–Ω—ã–µ –ø–ª–∞–Ω—ã —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ —É–∂–µ –¥–æ–∫—É–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã
- –°—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ –∏ —Å—Ö–µ–º—ã –≥–æ—Ç–æ–≤—ã –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é
- Edge Functions –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è—é—Ç –≥–æ—Ç–æ–≤—ã–µ —à–∞–±–ª–æ–Ω—ã

–≠—Ç–æ—Ç –ø–ª–∞–Ω –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω –ø–æ–¥ –º–æ–∏ MCP –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –∏ –ø–æ–∑–≤–æ–ª—è–µ—Ç –º–Ω–µ –≤—ã–ø–æ–ª–Ω—è—Ç—å –∑–∞–¥–∞—á–∏ –ø–æ—à–∞–≥–æ–≤–æ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –∫–∞–∂–¥–æ–º —ç—Ç–∞–ø–µ.

## üõ†Ô∏è –ü–û–õ–ù–´–ô –ò–ù–í–ï–ù–¢–ê–†–¨ MCP –ò–ù–°–¢–†–£–ú–ï–ù–¢–û–í

### üî¥ Redis MCP Server (44 –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞)
**–°—Ç–∞—Ç—É—Å**: ‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω –∏ –≥–æ—Ç–æ–≤ –∫ —Ä–∞–±–æ—Ç–µ  
**–ö–æ–º–∞–Ω–¥–∞**: `uvx --from git+https://github.com/redis/mcp-redis.git redis-mcp-server --url redis://default:redis_secure_password_2024_domain@95.111.252.29:6379/0`

```bash
# === STRING OPERATIONS (2 tools) ===
set                    # Set string value with optional expiration
get                    # Get string value by key

# === HASH OPERATIONS (8 tools) ===
hset                   # Set field in hash with optional expiration
hget                   # Get field value from hash
hgetall                # Get all fields and values from hash
hdel                   # Delete field from hash
hexists                # Check if field exists in hash
set_vector_in_hash     # Store vector as field in hash (for AI embeddings)
get_vector_from_hash   # Retrieve vector from hash (for AI similarity)

# === VECTOR SEARCH & INDEXING (5 tools) ===
get_indexes            # List all FT indices
get_index_info         # Get detailed index information
get_indexed_keys_number # Get number of indexed keys
create_vector_index_hash # Create HNSW vector index
vector_search_hash     # Perform KNN vector similarity search

# === LIST OPERATIONS (6 tools) ===
lpush, rpush, lpop, rpop, lrange, llen

# === SET OPERATIONS (3 tools) ===
sadd, srem, smembers

# === SORTED SET OPERATIONS (3 tools) ===
zadd, zrange, zrem

# === JSON OPERATIONS (3 tools) ===
json_set, json_get, json_del

# === PUB/SUB OPERATIONS (3 tools) ===
publish, subscribe, unsubscribe

# === STREAM OPERATIONS (3 tools) ===
xadd, xrange, xdel

# === MISCELLANEOUS OPERATIONS (6 tools) ===
delete, type, expire, rename, scan_keys, scan_all_keys

# === SERVER MANAGEMENT (2 tools) ===
dbsize, info, client_list
```

### üîµ Supabase MCP Server (19 –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤)
**–°—Ç–∞—Ç—É—Å**: ‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω –¥–ª—è –ø—Ä–æ–µ–∫—Ç–∞ bvcgsavjmrvkxcetyeyz  
**–ö–æ–º–∞–Ω–¥–∞**: `npx -y @supabase/mcp-server-supabase@latest --project-ref=bvcgsavjmrvkxcetyeyz`

```bash
# === DATABASE MANAGEMENT (8 tools) ===
list_tables            # List all tables in database
list_extensions        # List installed PostgreSQL extensions
execute_sql            # Execute SQL queries directly
get_logs               # Get database logs for debugging
get_advisors           # Get optimization recommendations
get_project_url        # Get Supabase project URL
get_anon_key           # Get anonymous access key
generate_typescript_types # Generate TypeScript types from schema

# === BRANCH MANAGEMENT (6 tools) ===
create_branch, list_branches, delete_branch, merge_branch, reset_branch, rebase_branch

# === MIGRATION MANAGEMENT (2 tools) ===
list_migrations, apply_migration

# === EDGE FUNCTIONS (2 tools) ===
list_edge_functions    # List deployed Edge Functions
deploy_edge_function   # Deploy new Edge Function

# === DOCUMENTATION (1 tool) ===
search_docs            # Search Supabase documentation
```

### üü¢ n8n MCP Server (39 –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤)
**–°—Ç–∞—Ç—É—Å**: ‚ùå –¢—Ä–µ–±—É–µ—Ç –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è (0 nodes available)  
**–ö–æ–º–∞–Ω–¥–∞**: `npx -y n8n-mcp`

```bash
# === CORE TOOLS (1 tool) ===
tools_documentation    # Get MCP tool documentation (START HERE!)

# === NODE DISCOVERY (4 tools) ===
list_nodes             # List all n8n nodes with filtering options
search_nodes           # Search nodes by keyword/functionality
list_ai_tools          # List AI-capable nodes (268 detected)
get_database_statistics # Get workflow database statistics

# === NODE CONFIGURATION (8 tools) ===
get_node_info          # Get complete node schema (100KB+)
get_node_essentials    # Get compact node overview (5KB)
search_node_properties # Search for specific node properties
get_node_for_task      # Get pre-configured node templates
get_property_dependencies # Get node property dependencies
get_node_as_tool_info  # Get node information as AI tool
list_node_templates    # List available node templates
get_template           # Get specific template

# === TASK MANAGEMENT (3 tools) ===
list_tasks             # List available workflow tasks
search_templates       # Search workflow templates
get_templates_for_task # Get templates for specific tasks

# === VALIDATION TOOLS (5 tools) ===
validate_node_operation # Full node validation with fixes
validate_node_minimal  # Required fields validation only
validate_workflow      # Complete workflow validation
validate_workflow_connections # Validate workflow connections
validate_workflow_expressions # Validate workflow expressions
```

### ‚ö° Additional MCP Servers

**Sequential Thinking MCP**
```bash
# Command: npx -y @modelcontextprotocol/server-sequential-thinking
# Advanced reasoning and problem-solving for complex tasks
```

**Context7 MCP**
```bash
# Command: npx -y @upstash/context7-mcp@latest
# Documentation and knowledge base access
```

**Custom AI Automation Server**
```bash
# Command: node mcp-servers/my-custom-server/server.js
# Custom project server (trust: true)
```

### üéØ –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –ø–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è:

**üî• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–µ –¥–ª—è Day 1:**
- `redis: info()` - –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å–æ—Å—Ç–æ—è–Ω–∏—è Redis
- `redis: get_indexes()` - —Å–ø–∏—Å–æ–∫ –∏–Ω–¥–µ–∫—Å–æ–≤  
- `supabase: list_tables()` - –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ö–µ–º—ã –ë–î
- `n8n: get_database_statistics()` - –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ n8n

**üìà –í—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏:**
- `redis: create_vector_index_hash()` - —Å–æ–∑–¥–∞–Ω–∏–µ HNSW –∏–Ω–¥–µ–∫—Å–æ–≤
- `redis: vector_search_hash()` - —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–∏—Å–∫–∞
- `supabase: execute_sql()` - —Å–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö —Ç–∞–±–ª–∏—Ü
- `supabase: deploy_edge_function()` - —Ä–∞–∑–≤–µ—Ä—Ç—ã–≤–∞–Ω–∏–µ LightRAG

**üîß –ò–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:**
- `redis: scan_all_keys()` - –∞–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö
- `supabase: get_advisors()` - —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
- `n8n: validate_workflow()` - –≤–∞–ª–∏–¥–∞—Ü–∏—è workflow

**üöÄ –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤: 107**
- Redis MCP: 44 –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞
- Supabase MCP: 19 –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤  
- n8n MCP: 39 –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤
- –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Å–µ—Ä–≤–µ—Ä—ã: 5+ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤

–≠—Ç–∞ –ø–æ–ª–Ω–∞—è –∏–Ω–≤–µ–Ω—Ç–∞—Ä–∏–∑–∞—Ü–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–Ω–∏–º–∞—Ç—å –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —Å–∏—Å—Ç–µ–º—ã –∏ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞—Ç—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞ –¥–ª—è –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è —Ü–µ–ª–µ–π –ø—Ä–æ–µ–∫—Ç–∞.

---

# üöÄ **–ù–ê–ß–ê–õ–û –í–´–ü–û–õ–ù–ï–ù–ò–Ø: AI-TEST WORKFLOW IMPLEMENTATION**

## üìã **Execution Log - Day 1**
**Timestamp**: Starting implementation  
**Focus**: Full n8n AI-Test workflow implementation  
**Status**: üü° In Progress

### üîç **Step 1: Infrastructure Diagnostics**
Validating ready infrastructure before AI-Test workflow creation:

**‚úÖ Redis MCP Server**: Ready with 44 tools (confirmed)  
**‚úÖ Supabase MCP Server**: Configured for project bvcgsavjmrvkxcetyeyz  
**‚ùå n8n MCP Server**: 0 nodes available (critical issue)

### üéØ **Step 2: AI-Test Workflow Implementation Strategy**

**Workflow Architecture:**
```
Webhook Input ‚Üí Content Processing ‚Üí Redis Vector Operations ‚Üí Supabase Storage ‚Üí AI Response
```

**Implementation Phases:**
1. **Infrastructure Setup**: Create Redis vector indices for AI-Test
2. **Workflow Creation**: Build n8n AI-Test workflow with existing tools
3. **Integration Testing**: Validate Redis ‚Üî Supabase ‚Üî n8n connections
4. **AI Enhancement**: Add contextual processing and LightRAG

### üöÄ **Step 3: Practical Implementation Execution**

#### **Phase 1A: Redis Vector Index Creation for AI-Test**

Using the ready Redis MCP Server to create optimized indices:

```bash
# Primary AI-Test document index
create_vector_index_hash(
    index_name="ai_test_docs",
    prefix="aitest:",
    vector_field="embedding",
    dim=1536,
    distance_metric="COSINE"
)

# AI-Test query cache index
create_vector_index_hash(
    index_name="ai_test_cache",
    prefix="cache:",
    vector_field="query_embedding",
    dim=1536,
    distance_metric="COSINE"
)
```

**Expected Results:**
- HNSW indices optimized for AI workloads
- Sub-50ms vector search performance
- Support for 1536-dimensional OpenAI embeddings

#### **Phase 1B: Supabase Schema Enhancement for AI-Test**

Extending existing schema with AI-Test specific tables:

```sql
-- AI-Test workflow results table
CREATE TABLE ai_test_results (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  test_type TEXT NOT NULL,
  input_content TEXT,
  processed_chunks JSONB,
  embeddings_generated INTEGER,
  redis_operations JSONB,
  final_response TEXT,
  performance_metrics JSONB,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- AI-Test embeddings cache
CREATE TABLE ai_test_embeddings (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  content_hash VARCHAR(64) UNIQUE,
  original_content TEXT,
  chunk_index INTEGER,
  embedding vector(1536),
  contextual_metadata JSONB,
  created_at TIMESTAMPTZ DEFAULT NOW()
);

-- Optimized indices for AI-Test
CREATE INDEX ai_test_embeddings_vector_idx 
ON ai_test_embeddings 
USING hnsw (embedding vector_cosine_ops) 
WITH (m = 64, ef_construction = 300);

CREATE INDEX ai_test_results_performance_idx 
ON ai_test_results 
USING gin (performance_metrics);
```

#### **Phase 2A: n8n AI-Test Workflow Creation**

**Workflow Structure:**
```json
{
  "name": "AI-Test Workflow",
  "active": true,
  "nodes": [
    {
      "name": "Webhook Trigger",
      "type": "n8n-nodes-base.webhook",
      "parameters": {
        "path": "ai-test",
        "httpMethod": "POST",
        "responseMode": "responseNode"
      },
      "position": [240, 300]
    },
    {
      "name": "Input Validation",
      "type": "n8n-nodes-base.code",
      "parameters": {
        "jsCode": "// Validate AI-Test input\nconst input = $input.first().json;\nif (!input.content) throw new Error('Content required');\nreturn { validatedInput: input, timestamp: new Date().toISOString() };"
      },
      "position": [460, 300]
    },
    {
      "name": "Redis Vector Index Setup",
      "type": "n8n-nodes-base.code",
      "parameters": {
        "jsCode": "// Check and create Redis vector indices for AI-Test\nconst indexConfig = {\n  name: 'ai_test_docs',\n  prefix: 'aitest:',\n  dim: 1536,\n  metric: 'COSINE'\n};\nreturn { indexConfig, status: 'ready' };"
      },
      "position": [680, 300]
    },
    {
      "name": "Contextual Chunking",
      "type": "n8n-nodes-base.code",
      "parameters": {
        "jsCode": "// AI-Test contextual chunking with 800-token chunks\nconst content = $input.first().json.validatedInput.content;\nconst chunks = contextualChunk(content, 800);\nreturn { chunks, totalChunks: chunks.length };"
      },
      "position": [900, 300]
    },
    {
      "name": "Generate Embeddings",
      "type": "n8n-nodes-base.openAi",
      "parameters": {
        "operation": "embedding",
        "model": "text-embedding-3-large",
        "dimensions": 1536
      },
      "position": [1120, 300]
    },
    {
      "name": "Store in Redis L1",
      "type": "n8n-nodes-base.code",
      "parameters": {
        "jsCode": "// Store embeddings in Redis using MCP tools\nconst embeddings = $input.first().json.embeddings;\nembeddings.forEach((emb, idx) => {\n  redis.hSet(`aitest:doc_${Date.now()}_${idx}`, {\n    embedding: emb.vector,\n    content: emb.content,\n    metadata: JSON.stringify(emb.metadata)\n  });\n});\nreturn { redisStored: embeddings.length };"
      },
      "position": [1340, 300]
    },
    {
      "name": "Store in Supabase L2",
      "type": "n8n-nodes-base.supabase",
      "parameters": {
        "operation": "insert",
        "table": "ai_test_embeddings"
      },
      "position": [1560, 300]
    },
    {
      "name": "Vector Search Test",
      "type": "n8n-nodes-base.code",
      "parameters": {
        "jsCode": "// Test vector search using Redis MCP\nconst testVector = $input.first().json.embeddings[0].vector;\nconst searchResults = await redis.vectorSearchHash(testVector, 'ai_test_docs', 5);\nreturn { searchResults, testPassed: searchResults.length > 0 };"
      },
      "position": [1780, 300]
    },
    {
      "name": "AI Response Generation",
      "type": "n8n-nodes-base.openAi",
      "parameters": {
        "operation": "completion",
        "model": "gpt-4",
        "messages": [
          {
            "role": "system",
            "content": "You are an AI assistant analyzing test results. Provide insights based on the vector search and embedding results."
          }
        ]
      },
      "position": [2000, 300]
    },
    {
      "name": "Response",
      "type": "n8n-nodes-base.respond",
      "parameters": {
        "responseCode": 200
      },
      "position": [2220, 300]
    }
  ],
  "connections": {
    "Webhook Trigger": { "main": [[{ "node": "Input Validation", "type": "main", "index": 0 }]] },
    "Input Validation": { "main": [[{ "node": "Redis Vector Index Setup", "type": "main", "index": 0 }]] },
    "Redis Vector Index Setup": { "main": [[{ "node": "Contextual Chunking", "type": "main", "index": 0 }]] },
    "Contextual Chunking": { "main": [[{ "node": "Generate Embeddings", "type": "main", "index": 0 }]] },
    "Generate Embeddings": { "main": [[{ "node": "Store in Redis L1", "type": "main", "index": 0 }]] },
    "Store in Redis L1": { "main": [[{ "node": "Store in Supabase L2", "type": "main", "index": 0 }]] },
    "Store in Supabase L2": { "main": [[{ "node": "Vector Search Test", "type": "main", "index": 0 }]] },
    "Vector Search Test": { "main": [[{ "node": "AI Response Generation", "type": "main", "index": 0 }]] },
    "AI Response Generation": { "main": [[{ "node": "Response", "type": "main", "index": 0 }]] }
  }
}
```

#### **Phase 2B: Integration Testing Plan**

**Test Scenarios:**
1. **Basic Input Processing**: Send test content via webhook
2. **Redis Integration**: Verify vector index creation and search
3. **Supabase Integration**: Confirm data persistence and retrieval
4. **End-to-End Flow**: Complete workflow validation
5. **Performance Testing**: Measure latency and accuracy

**Test Payload Example:**
```json
{
  "content": "This is a test document for AI processing. It contains multiple sentences that will be chunked, embedded, and stored in both Redis and Supabase for vector similarity search testing.",
  "metadata": {
    "type": "ai-test",
    "source": "manual",
    "priority": "high"
  }
}
```

### üìã **Current Implementation Status:**

**‚úÖ Completed:**
- Infrastructure analysis and validation
- AI-Test workflow architecture design
- Redis vector index configuration
- Supabase schema enhancements
- Complete n8n workflow JSON structure

**üü° In Progress:**
- Workflow deployment and testing
- MCP tool integration validation
- Performance optimization

**üî¥ Pending:**
- n8n connectivity restoration
- Live workflow deployment
- Production optimization

### üéØ **Next Steps for Full Deployment:**

1. **Resolve n8n MCP connectivity** to enable workflow creation
2. **Deploy Redis vector indices** using the configured MCP tools
3. **Create Supabase tables** for AI-Test data storage
4. **Import and activate** the AI-Test workflow in n8n
5. **Execute end-to-end testing** with real data
6. **Optimize performance** based on test results

---

# üöÄ **LIVE EXECUTION LOG - FULL PLAN IMPLEMENTATION**
**Started**: Now  
**Status**: üî¥ Active Implementation  
**Target**: Complete AI automation platform deployment with AI-Test workflow

## **Phase 1: Infrastructure Validation & Setup**

### **1.1 Redis MCP Server Assessment**
**Executing**: Checking Redis 8.2.1 status and creating vector indices...

**Commands to Execute Now:**
```bash
# 1. Check Redis connectivity and status
info()

# 2. List current indices (should be empty initially)
get_indexes()

# 3. Check database size
dbsize()

# 4. Get server info
info("server")
```

**Expected Results:**
- Redis 8.2.1 RC running
- 0 existing indices
- Empty database (0 keys)
- Modules: RediSearch, ReJSON, Bloom loaded

### **1.2 Supabase MCP Server Assessment**
**Executing**: Validating database schema and connectivity...

**Commands to Execute:**
```bash
# 1. Verify project connectivity
get_project_url()

# 2. List existing tables
list_tables()

# 3. Check installed extensions
list_extensions()

# 4. Get performance advisors
get_advisors()
```

**Expected Results:**
- Project URL: bvcgsavjmrvkxcetyeyz.supabase.co
- Existing tables: documents_v2, opencart_products, user_query_history
- Extensions: pgvector 0.8.0, pg_trgm, pgcrypto
- Performance recommendations available

### **1.3 n8n MCP Server Assessment**
**Executing**: Checking workflow engine connectivity...

**Commands to Execute:**
```bash
# 1. Check n8n health
get_database_statistics()

# 2. List available nodes
list_nodes()

# 3. Get AI-capable tools
list_ai_tools()

# 4. Check for existing workflows
tools_documentation()
```

**Expected Results:**
- Current issue: 0 nodes available (connectivity problem)
- Should show 525+ nodes when working
- 268 AI-capable nodes available
- MCP tool documentation accessible

---

## **Phase 2: Redis Vector Infrastructure Setup**

### **2.1 Creating AI-Test Vector Indices**
**Executing**: Setting up optimized HNSW indices for AI workloads...

**Commands to Execute:**
```bash
# Create primary document index for AI-Test
create_vector_index_hash(
    index_name="ai_test_docs",
    prefix="aitest:",
    vector_field="embedding",
    dim=1536,
    distance_metric="COSINE"
)

# Create query cache index
create_vector_index_hash(
    index_name="ai_test_cache",
    prefix="cache:",
    vector_field="query_embedding",
    dim=1536,
    distance_metric="COSINE"
)

# Create product index (leveraging existing opencart_products)
create_vector_index_hash(
    index_name="product_vectors",
    prefix="product:",
    vector_field="embedding",
    dim=1536,
    distance_metric="COSINE"
)
```

**Validation Commands:**
```bash
# Verify indices creation
get_indexes()

# Check detailed index info
get_index_info("ai_test_docs")
get_index_info("ai_test_cache")
get_index_info("product_vectors")

# Check indexed keys count
get_indexed_keys_number("ai_test_docs")
```

**Expected Results:**
- 3 new vector indices created
- HNSW algorithm with COSINE distance
- 1536-dimensional vector support
- Ready for AI embedding storage

### **2.2 Testing Vector Operations**
**Executing**: Validating vector storage and search capabilities...

**Test Commands:**
```bash
# Store test vector in hash
set_vector_in_hash(
    name="aitest:test_doc_1",
    vector=[0.1, 0.2, 0.3, ...], # 1536 dimensions
    vector_field="embedding"
)

# Store additional metadata
hset(
    name="aitest:test_doc_1",
    key="content",
    value="This is a test document for AI processing validation."
)

hset(
    name="aitest:test_doc_1",
    key="metadata",
    value='{"type": "test", "created": "2025-01-27"}'
)

# Test vector search
vector_search_hash(
    query_vector=[0.1, 0.2, 0.3, ...], # Similar vector
    index_name="ai_test_docs",
    k=5
)

# Retrieve stored vector
get_vector_from_hash(
    name="aitest:test_doc_1",
    vector_field="embedding"
)
```

**Expected Results:**
- Vector successfully stored in Redis hash
- Search returns similarity matches
- Sub-50ms search performance
- Vector retrieval works correctly

---

## **Phase 3: Supabase Schema Deployment**

### **3.1 Creating AI-Test Tables**
**Executing**: Deploying enhanced schema for AI workflows...

**SQL Commands to Execute:**
```sql
-- Create AI-Test results tracking table
execute_sql("
CREATE TABLE IF NOT EXISTS ai_test_results (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  test_type TEXT NOT NULL,
  input_content TEXT,
  processed_chunks JSONB,
  embeddings_generated INTEGER,
  redis_operations JSONB,
  final_response TEXT,
  performance_metrics JSONB,
  created_at TIMESTAMPTZ DEFAULT NOW()
);
")

-- Create AI-Test embeddings cache table
execute_sql("
CREATE TABLE IF NOT EXISTS ai_test_embeddings (
  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
  content_hash VARCHAR(64) UNIQUE,
  original_content TEXT,
  chunk_index INTEGER,
  embedding vector(1536),
  contextual_metadata JSONB,
  created_at TIMESTAMPTZ DEFAULT NOW()
);
")

-- Create optimized HNSW index
execute_sql("
CREATE INDEX IF NOT EXISTS ai_test_embeddings_vector_idx 
ON ai_test_embeddings 
USING hnsw (embedding vector_cosine_ops) 
WITH (m = 64, ef_construction = 300);
")

-- Create performance index
execute_sql("
CREATE INDEX IF NOT EXISTS ai_test_results_performance_idx 
ON ai_test_results 
USING gin (performance_metrics);
")

-- Enhance existing documents_v2 for contextual retrieval
execute_sql("
ALTER TABLE documents_v2 
ADD COLUMN IF NOT EXISTS contextual_content TEXT,
ADD COLUMN IF NOT EXISTS contextual_fts TSVECTOR 
GENERATED ALWAYS AS (to_tsvector('english', contextual_content)) STORED;
")

-- Create contextual FTS index
execute_sql("
CREATE INDEX IF NOT EXISTS documents_v2_contextual_fts_idx 
ON documents_v2 
USING gin (contextual_fts);
")
```

**Validation Commands:**
```bash
# Verify new tables created
list_tables()

# Check table structure
execute_sql("SELECT column_name, data_type FROM information_schema.columns WHERE table_name = 'ai_test_results';")

# Verify indices
execute_sql("SELECT indexname, tablename FROM pg_indexes WHERE tablename LIKE 'ai_test%';")

# Test vector operations
execute_sql("INSERT INTO ai_test_embeddings (content_hash, original_content, embedding) VALUES ('test123', 'Test content', '[0.1,0.2,0.3]'::vector(3));")
```

### **3.2 Edge Function Enhancement**
**Executing**: Deploying LightRAG Edge Function...

**Edge Function Code:**
```typescript
// File: supabase/functions/light-rag-processor/index.ts
import { serve } from "https://deno.land/std@0.168.0/http/server.ts";
import OpenAI from 'https://deno.land/x/openai@v4.24.0/mod.ts';

interface LightRAGRequest {
  query: string;
  documents: any[];
  mode?: 'simple' | 'hybrid';
  max_results?: number;
}

serve(async (req) => {
  try {
    const { query, documents, mode = 'hybrid', max_results = 10 }: LightRAGRequest = await req.json();
    
    console.log(`Processing LightRAG request: mode=${mode}, docs=${documents.length}`);
    
    // Initialize OpenAI client
    const apiKey = Deno.env.get('OPENAI_API_KEY');
    if (!apiKey) {
      throw new Error('OPENAI_API_KEY not configured');
    }
    
    const openai = new OpenAI({ apiKey });
    
    // Generate query embedding
    const queryEmbedding = await openai.embeddings.create({
      model: "text-embedding-3-large",
      input: query,
      dimensions: 1536,
    });
    
    // Process documents based on mode
    let processedResults;
    
    if (mode === 'hybrid') {
      // Combine vector similarity with contextual analysis
      processedResults = await processHybridSearch({
        query,
        queryVector: queryEmbedding.data[0].embedding,
        documents,
        maxResults: max_results
      });
    } else {
      // Simple vector similarity
      processedResults = await processSimpleSearch({
        query,
        queryVector: queryEmbedding.data[0].embedding,
        documents,
        maxResults: max_results
      });
    }
    
    return new Response(
      JSON.stringify({
        success: true,
        mode,
        results: processedResults,
        processing_time: Date.now(),
        total_documents: documents.length
      }),
      { headers: { "Content-Type": "application/json" } }
    );
    
  } catch (error) {
    console.error('LightRAG processing error:', error);
    
    return new Response(
      JSON.stringify({
        success: false,
        error: error.message,
        timestamp: new Date().toISOString()
      }),
      { 
        status: 500,
        headers: { "Content-Type": "application/json" } 
      }
    );
  }
});

// Hybrid search implementation
async function processHybridSearch({ query, queryVector, documents, maxResults }) {
  // Implement graph-based contextual search
  // This would integrate with your existing documents_v2 and implement
  // the LightRAG 2.0 algorithm for improved retrieval accuracy
  
  return documents
    .map((doc, index) => ({
      ...doc,
      similarity_score: calculateCosineSimilarity(queryVector, doc.embedding || []),
      contextual_score: calculateContextualRelevance(query, doc.content || ''),
      hybrid_score: 0.7 * calculateCosineSimilarity(queryVector, doc.embedding || []) + 
                   0.3 * calculateContextualRelevance(query, doc.content || '')
    }))
    .sort((a, b) => b.hybrid_score - a.hybrid_score)
    .slice(0, maxResults);
}

// Simple search implementation
async function processSimpleSearch({ query, queryVector, documents, maxResults }) {
  return documents
    .map(doc => ({
      ...doc,
      similarity_score: calculateCosineSimilarity(queryVector, doc.embedding || [])
    }))
    .sort((a, b) => b.similarity_score - a.similarity_score)
    .slice(0, maxResults);
}

// Utility functions
function calculateCosineSimilarity(vecA: number[], vecB: number[]): number {
  if (vecA.length !== vecB.length) return 0;
  
  const dotProduct = vecA.reduce((sum, a, i) => sum + a * vecB[i], 0);
  const magnitudeA = Math.sqrt(vecA.reduce((sum, a) => sum + a * a, 0));
  const magnitudeB = Math.sqrt(vecB.reduce((sum, b) => sum + b * b, 0));
  
  return magnitudeA && magnitudeB ? dotProduct / (magnitudeA * magnitudeB) : 0;
}

function calculateContextualRelevance(query: string, content: string): number {
  // Simple keyword-based relevance (can be enhanced with more sophisticated NLP)
  const queryTerms = query.toLowerCase().split(/\s+/);
  const contentTerms = content.toLowerCase().split(/\s+/);
  
  const matches = queryTerms.filter(term => contentTerms.includes(term));
  return matches.length / queryTerms.length;
}
```

**Deploy Command:**
```bash
deploy_edge_function(
  function_name="light-rag-processor",
  source_code=// Above TypeScript code
)
```

---

## **Phase 4: n8n AI-Test Workflow Deployment**

### **4.1 Workflow Creation and Import**

**Note**: This phase requires n8n MCP connectivity to be restored first.

**Once n8n is accessible, execute:**

```bash
# 1. Start with documentation
tools_documentation()

# 2. Search for required nodes
search_nodes("webhook")
search_nodes("openai")
search_nodes("code")
search_nodes("supabase")

# 3. Get node essentials for workflow building
get_node_essentials("n8n-nodes-base.webhook")
get_node_essentials("n8n-nodes-base.openai")
get_node_essentials("n8n-nodes-base.code")

# 4. Create the complete AI-Test workflow
# (Use the JSON structure provided in the design above)
```

**Workflow Validation:**
```bash
# Validate workflow before deployment
validate_workflow(workflow_json)

# Validate connections
validate_workflow_connections(workflow_json)

# Validate expressions
validate_workflow_expressions(workflow_json)
```

### **4.2 Integration Testing**

**Test Execution:**
```bash
# Test webhook endpoint
POST /webhook/ai-test
{
  "content": "This is a test document for AI processing. It demonstrates the complete workflow from input validation through contextual chunking, embedding generation, dual storage in Redis and Supabase, vector search testing, and AI-powered response generation.",
  "metadata": {
    "type": "integration_test",
    "source": "implementation_validation",
    "priority": "high",
    "test_id": "ai_test_001"
  }
}
```

**Expected Flow:**
1. Webhook receives POST request
2. Input validation passes
3. Redis vector index confirmed ready
4. Content chunked into 800-token segments
5. OpenAI generates 1536-dim embeddings
6. Embeddings stored in Redis L1 cache
7. Data persisted to Supabase L2 storage
8. Vector search test executed
9. AI generates response based on results
10. Complete response returned

**Performance Targets:**
- **Total latency**: <50ms
- **Vector search**: <10ms
- **Embedding generation**: <2000ms
- **Database operations**: <500ms
- **Success rate**: >99%

---

## **Current Implementation Progress:**

**‚úÖ READY FOR EXECUTION:**
- Redis vector index creation commands
- Supabase schema deployment SQL
- LightRAG Edge Function code
- Complete n8n workflow JSON
- Integration testing plan
- Performance validation framework

**üîÑ EXECUTION SEQUENCE:**
1. Run Redis commands above (Phase 2)
2. Execute Supabase SQL commands (Phase 3.1)
3. Deploy LightRAG Edge Function (Phase 3.2)
4. Resolve n8n connectivity and import workflow (Phase 4.1)
5. Execute integration tests (Phase 4.2)
6. Monitor and optimize performance

**üéØ SUCCESS CRITERIA:**
- All vector indices created and operational
- Supabase schema enhanced with AI-Test tables
- LightRAG Edge Function deployed and accessible
- AI-Test workflow active and processing requests
- Performance targets achieved
- End-to-end validation successful

---

# üî• **–†–ï–ê–õ–¨–ù–û–ï –í–´–ü–û–õ–ù–ï–ù–ò–ï –ü–õ–ê–ù–ê - LIVE EXECUTION**
**–ù–∞—á–∞—Ç–æ**: –°–µ–π—á–∞—Å  
**–°—Ç–∞—Ç—É—Å**: üü¢ –ê–∫—Ç–∏–≤–Ω–æ–µ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ  
**–¶–µ–ª—å**: –ü–æ–ª–Ω–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è AI automation platform —Å AI-Test workflow

## **–§–∞–∑–∞ 1: –î–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ –∏–Ω—Ñ—Ä–∞—Å—Ç—Ä—É–∫—Ç—É—Ä—ã**

### **–®–∞–≥ 1.1: –ü—Ä–æ–≤–µ—Ä–∫–∞ Redis MCP —Å–µ—Ä–≤–µ—Ä–∞**
üîç **–í—ã–ø–æ–ª–Ω—è—é –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫—É Redis 8.2.1...**